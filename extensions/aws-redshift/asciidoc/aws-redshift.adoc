== Optional Lab: Stream Sales & Purchases to AWS Redshift

We can use the JDBC Sink Connector to stream changes from our topics to Google BigQuery.

To do this we'll use the KSQL CLI to create the connector.

Start a KSQL CLI session
[source,bash,subs=attributes]
----
docker exec -it ksql-cli ksql http://ksql-server-ccloud:8088
----

And run the following `CREATE SINK CONNECTOR` command. This will create a connector that will stream the following topics to AWS Redshift:-

[source,bash,subs=attributes]
----
{dc}_sales_orders
{dc}_sales_order_details
{dc}_purchase_orders
{dc}_purchase_order_details
{dc}_products
{dc}_customers
{dc}_suppliers 
----

[source,bash,subs=attributes]
----
CREATE SINK CONNECTOR {dc}_redshift_sink WITH (  
  'connector.class'='io.confluent.connect.jdbc.JdbcSinkConnector',
  'topics'='{dc}_sales_orders,{dc}_sales_order_details,{dc}_purchase_orders,{dc}_purchase_order_details,{dc}_products,{dc}_customers,{dc}_suppliers',
  'connection.url'='${file:/secrets.properties:RS_JDBC_URL}',
  'connection.user'='${file:/secrets.properties:RS_USERNAME}',
  'connection.password'='${file:/secrets.properties:RS_PASSWORD}',
  'insert.mode'='INSERT',
  'batch.size'='20',
  'auto.create'='true',
  'key.converter'='org.apache.kafka.connect.storage.StringConverter',
  'dialect.name'='PostgreSqlDatabaseDialect'
);

----

We can list our current connectors using the following command

[source,bash,subs=attributes]
----
show connectors;
----

[source,bash,subs=attributes]
----
 Connector Name            | Type   | Class
------------------------------------------------------------------------------------------------
 DC01_REDSHIFT_SINK        | SINK   | io.confluent.connect.jdbc.JdbcSinkConnector
 replicator-dc01-to-ccloud | SOURCE | io.confluent.connect.replicator.ReplicatorSourceConnector
------------------------------------------------------------------------------------------------

----

We can also describe a connector and view its status using the `describe connector` statement.

[source,bash,subs=attributes]
----
describe connector DC01_REDSHIFT_SINK;
----
[source,bash,subs=attributes]
----
Name                 : DC01_REDSHIFT_SINK
Class                : io.confluent.connect.jdbc.JdbcSinkConnector
Type                 : sink
State                : RUNNING
WorkerId             : kafka-connect:18084

 Task ID | State   | Error Trace
---------------------------------
 0       | RUNNING |
---------------------------------
----

Depending on who's hosting the workshop, you may or may not have access to the AWS account where the Redshift cluster is hosted.

