== Optional Lab: Stream Sales & Purchases to Azure Blob Storage

We can use the Azure Blob Storage Sink Connector to stream changes from a topics to Azure Blob Storage, from here the data can be consumed other Azure services.

Another preview feature of KSQL 5.4 is the ability to create connectors directly within KSQL.

Start a KSQL CLI session
[source,bash,subs=attributes]
----
docker exec -it ksql-cli ksql http://ksql-server-ccloud:8088
----

And run the following `CREATE SINK CONNECTOR` command. This will create a connector that will sink the `{dc}_sales_enriched` and the `{dc}_purchases_enriched` topics to Azure Blob Storage.

[source,bash,subs=attributes]
----
CREATE SINK CONNECTOR {dc}_azure_blob_sink WITH (
  'connector.class'= 'io.confluent.connect.azure.blob.AzureBlobStorageSinkConnector',
  'tasks.max'= '1',
  'topics'= '{dc}_sales_enriched,{dc}_purchases_enriched',
  'topics.dir'= '{dc}_topics',
  'flush.size'= '20',
  'azblob.account.name'= '${file:/secrets.properties:AZURE_STORAGE_ACCOUNT_NAME}',
  'azblob.account.key'= '${file:/secrets.properties:AZURE_STORAGE_ACCOUNT_KEY}',
  'azblob.container.name'= '${file:/secrets.properties:AZURE_STORAGE_CONTAINER}',
  'format.class'= 'io.confluent.connect.azure.blob.format.avro.AvroFormat',
  'confluent.topic.bootstrap.servers'= '${file:/secrets.properties:CCLOUD_CLUSTER_ENDPOINT}',
  'confluent.topic.security.protocol' = 'SASL_SSL',
  'confluent.topic.sasl.mechanism' = 'PLAIN',
  'confluent.topic.sasl.jaas.config' = 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"${file:/secrets.properties:CCLOUD_API_KEY}\" password=\"${file:/secrets.properties:CCLOUD_API_SECRET}\";',
  'confluent.topic.replication.factor'= '3',
  'key.converter'='org.apache.kafka.connect.storage.StringConverter'
);
----

We can list our current connectors using the following command

[source,bash,subs=attributes]
----
show connectors;
----

[source,bash,subs=attributes]
----
 Connector Name            | Type   | Class
------------------------------------------------------------------------------------------------
 {dc}_AZURE_BLOB_SINK      | SINK   | io.confluent.connect.azure.blob.AzureBlobStorageSinkConnector
 replicator-{dc}-to-ccloud | SOURCE | io.confluent.connect.replicator.ReplicatorSourceConnector
------------------------------------------------------------------------------------------------
----

We can also describe a connector and view its status using the `describe connector` statement.

[source,bash,subs=attributes]
----
describe connector {dc}_AZURE_BLOB_SINK;
----
[source,bash,subs=attributes]
----
Name                 : {dc}_AZURE_BLOB_SINK
Class                : io.confluent.connect.gcs.GcsSinkConnector
Type                 : sink
State                : RUNNING
WorkerId             : kafka-connect:18084

 Task ID | State   | Error Trace
---------------------------------
 0       | RUNNING |
---------------------------------
----

Depending on who's hosting the workshop, you may or may not have access to the GCP account where the storage bucket is held.

.Further Reading
[NOTE]
====
* link:https://docs.confluent.io/current/connect/kafka-connect-azure-blob-storage/index.html[Azure Blob Storage Sink Connector]
* link:https://docs.confluent.io/current/connect/kafka-connect-azure-blob-storage/configuration_options.html#configuration-properties[Azure Blob Storage Sink Connector Configuration Properties]
====

