== Optional Lab: Stream Sales & Purchases to Google Cloud Storage

We can use the Google Cloud Storage Sink Connector to stream changes from a topics to Google Cloud Storage, from here the data can be consumed other Google Cloud Platform services.

Another preview feature of KSQL 5.4 is the ability to create connectors directly within KSQL.

Start a KSQL CLI session
[source,bash,subs=attributes]
----
docker exec -it ksql-cli ksql http://ksql-server-ccloud:8088
----

And run the following `CREATE SINK CONNECTOR` command. This will create a connector that will sink the `{dc}_sales_enriched` and the `{dc}_purchases_enriched` topics to Google Cloud Storage.

[source,bash,subs=attributes]
----
CREATE SINK CONNECTOR {dc}_gcs_sink WITH (
  'connector.class' = 'io.confluent.connect.gcs.GcsSinkConnector',
  'tasks.max'= '1',
  'gcs.bucket.name' = '${file:/secrets.properties:GCS_BUCKET_NAME}',
  'gcs.part.size' = '5242880',
  'gcs.credentials.path' = '${file:/secrets.properties:GCS_CREDENTIALS_PATH}',
  'flush.size'= '50',
  'storage.class'= 'io.confluent.connect.gcs.storage.GcsStorage',
  'format.class'= 'io.confluent.connect.gcs.format.avro.AvroFormat',
  'partitioner.class'= 'io.confluent.connect.storage.partitioner.DefaultPartitioner',
  'schema.compatibility'= 'NONE',
  'topics' = '{dc}_sales_enriched,{dc}_purchases_enriched',
  'confluent.topic.bootstrap.servers'= '${file:/secrets.properties:CCLOUD_CLUSTER_ENDPOINT}',
  'confluent.topic.security.protocol' = 'SASL_SSL',
  'confluent.topic.sasl.mechanism' = 'PLAIN',
  'confluent.topic.sasl.jaas.config' = 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"${file:/secrets.properties:CCLOUD_API_KEY}\" password=\"${file:/secrets.properties:CCLOUD_API_SECRET}\";',
  'confluent.topic.replication.factor'= '3',
  'producer.interceptor.classes' = 'io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor',
  'key.converter'= 'org.apache.kafka.connect.storage.StringConverter'
);
----

We can list our current connectors using the following command

[source,bash,subs=attributes]
----
show connectors;
----

[source,bash,subs=attributes]
----
 Connector Name            | Type   | Class
------------------------------------------------------------------------------------------------
 DC01_GCS_SINK             | SINK   | io.confluent.connect.gcs.GcsSinkConnector
 replicator-dc01-to-ccloud | SOURCE | io.confluent.connect.replicator.ReplicatorSourceConnector
------------------------------------------------------------------------------------------------
----

We can also describe a connector and view its status using the `describe connector` statement.

[source,bash,subs=attributes]
----
describe connector DC01_GCS_SINK;
----
[source,bash,subs=attributes]
----
Name                 : DC01_GCS_SINK
Class                : io.confluent.connect.gcs.GcsSinkConnector
Type                 : sink
State                : RUNNING
WorkerId             : kafka-connect:18084

 Task ID | State   | Error Trace
---------------------------------
 0       | RUNNING |
---------------------------------
----

Depending on who's hosting the workshop, you may or may not have access to the GCP account where the storage bucket is held.

.Further Reading
[TIP]
====
* link:https://docs.confluent.io/current/connect/kafka-connect-gcs/index.html#google-cloud-storage-sink-connector-for-cp[Google Cloud Storage Sink Connector]
* link:https://docs.confluent.io/current/connect/kafka-connect-gcs/configuration_options.html[Google Cloud Storage Sink Connector Configuration Properties]
====